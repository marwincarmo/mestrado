
# Method

## Participants and Study Design

```{r power-analysis}
ss_closefit_dbas <- semTools::findRMSEAsamplesize(rmsea0=.05, rmseaA=.059, df=98, power=.80)
ss_notclosefit_dbas <- semTools::findRMSEAsamplesize(rmsea0 = .08, rmseaA = .059, df=98, power=.80)

ss_closefit_spaq <- semTools::findRMSEAsamplesize(rmsea0=.05, rmseaA=.08, df=19, power=.80)
ss_notclosefit_spaq <- semTools::findRMSEAsamplesize(rmsea0 = .10, rmseaA = .08, df=19, power=.80)

```

To estimate an adequate sample size for the confirmatory factor analyses (CFAs) we used MacCallum et al.’s [-@maccallum1996] root-mean-square error of approximation (RMSEA) tests of close and not-close fit. All tests were conducted in R 4.1.3 [@R-base] using `semTools` version `r packageVersion("semTools")` [@semtools]. Morin [-@morin2007a] reports a RMSEA of 0.059 in a CFA for DBAS-16. Taking this value as prior guess for the true RMSEA score, we calculated the sample sizes required to to reject the test for not-close fit of RMSEA > 0.08 and the test of close fit of RMSEA < 0.05 with a power of 0.80 and $\alpha$ of 0.05. Results show that `r ss_notclosefit_dbas` subjects are necessary to reject the test for not-close fit, and `r ss_closefit_dbas` participants are required for rejection of the test of close fit. Therefore, we aimed at a minimum sample size of `r ss_closefit_dbas` participants. SPAQ's fit index was not considered in this power analysis due to the large RMSEA (0.081) reported originally [@bothelius2015].

This study was approved by the Ethics Committee of the General Hospital of the University of São Paulo, School of Medicine (HC-FMUSP), São Paulo, Brazil (CAAE: 46284821.1.0000.0068). Inclusion criteria was age between 18 and 59 years and reporting no difficulties in reading or writing in Portuguese. 

Participants were recruited mainly from advertisement on the internet, especially on HC-FMUSP's social media platforms (Instagram and Facebook). The data collection took place between May 2021 through July 2022, with brief breaks in between. Because the measures evaluated in this study refer to sleep difficulties, we sought to include participants both with and without insomnia complaints. The first group was composed by people registered for an experimental behavioral treatment for insomnia, which was also organized by the Department of Psychiatry of HC-FMUSP and which this study is a branch of. To recruit participants without insomnia complaints we asked for volunteer participation from people believing not having sleeping problems.

Bad sleepers were classified according to the presence of insomnia complaints: (i) difficulty initiating and/or maintaining sleep, defined as a sleep onset latency and/or wake after sleep onset greater than or equal to 30 minutes, with a corresponding sleep time of less than or equal to six hours per night; (ii) presence of insomnia for more than three nights per week and more than three months; (iii) sleep disturbance (or associated daytime fatigue) causing significant distress or impairment in social, occupational, or other areas of functioning. This definition represents a combination of criteria from the American Academy of Sleep Medicine, the International Classification of Sleep Disorders, and the Diagnostic and Statistical Manual of Mental Disorders, along with quantitative cutoffs typically used in insomnia research [@icds2014; @americanpsychiatricassociation2013; @edinger2004]. In addition to these criteria, participants total score on the Insomnia Severity Index should not exceed 7 points [@bastien2001].

Participants were informed about the main objective of the research and signed the informed consent. They were informed that their answers would be kept confidential, and that all procedures
guaranteeing the privacy of their results would be adopted. Then, they were requested to respond to an online survey using REDCap electronic data capture tools [@harris2009research; @harris2019redcap], including the Brazilian-Portuguese versions of DBAS-16 and SPAQ and other auxiliary instruments.

### Item translation

We mainly based our methods on Beaton's [-@beaton2000] recommendations with the addition of more up to date insights from @borsaAdaptacaoValidacaoInstrumentos2012. The following procedures were applied both to DBAS-16 as well as to SPAQ. Only the expert committee and the first translation team had a different configuration for each instrument.
Figure \@ref(fig:flowcca2) summarize the steps taken in the process.

In the first stage the items of the original versions were translated from English (source language) to Portuguese (target language) by three independent translators, of which two were familiar with the instrument constructs and the other English teachers unaware of the instrument concepts and with no clinical or medical background. The three versions were synthesized by an expert committee of health professionals experts in insomnia. A form adapted from @koller2012 was given to each member of the committee to register the rationale for the decisions (see Appendix \ref{appendix:form}). Then, two independent translators native speakers of the source language back translated the synthesized version to English. We reconciled the back translations into a single version and submitted it to appreciation by both first authors of the original questionnaires. Together with the expert committee we debated over suggestions raised by the original authors and made changes accordingly to the translated version. 

At the final step, we conducted a pilot study with 15 participants from the target population to probe the pre-final version. There were 12 female participants and overall mean age was 43 years (range: 19--57 years). To prevent restricting feedback to specific regional contexts [@borsaAdaptacaoValidacaoInstrumentos2012], we recruited participants from the five Brazilian regions and with varying educational levels. We were able to interview nine participants from the Southwest region, three from South, two from Northeast and one from Middle-west. We conducted individual cognitive interviews with each participant.

```{r flowcca, eval=FALSE, fig.align='center', fig.cap='Stages of cross-cultural adaptation. Adapted from “Cross-Cultural Adaptation and Validation of Psychological Instruments: Some Considerations”', message=FALSE, include=FALSE}
knitr::include_graphics("cca_diagram.pdf")
```

```{r flowcca2, echo=FALSE, fig.align='center', fig.cap='Stages of cross-cultural adaptation. Adapted from “Cross-Cultural Adaptation and Validation of Psychological Instruments: Some Considerations”', message=FALSE,  fig.align = 'center', out.width="100%", fig.width=10, fig.height=8}
DiagrammeR::grViz("digraph {
  graph [layout = dot, rankdir = TB]
  
  node [shape = rectangle] 
  init [label = 'Original instrument']
  ft1 [label = 'Translator 1 \n(informed translator)']
  ft2 [label = 'Translator 2 \n(informed translator)']
  ft3 [label = 'Translator 3 \n(uninformed translator)']
  synth [label = 'Synthesyzed \ntranslation']
  expcom [label = 'Expert committee \nreview']
  bkt1 [label = 'Backtranslator 1 \n(uninformed translator)']
  bkt2 [label = 'Backtranslator 2 \n(uninformed translator)']
  synthb [label = 'Synthesyzed \nbacktranslation']
  pog [label = 'Presentation of the new version \nto the original authors']
  adq [label = 'Adequate?']
  disc [label = 'Discussion with the \noriginal authors']
  
  
  # edge definitions with the node IDs
  init -> {ft1 ft2 ft3}
  {ft1 ft2 ft3} -> synth
  synth -> expcom -> {bkt1 bkt2}
  {bkt1 bkt2} -> synthb
  synthb -> pog
  {rank=same; pog; adq}
  pog -> adq 
  adq -> {Yes No}
  {rank=same; No; disc}
  No -> disc -> adq
  Yes -> 'Pilot Study' -> 'Expert committee \nreview' -> 'Final translated \n version'
  }",
  height = 1680, 
  width = 800
  )
```


## Aditional measures

1. *Insomnia Severity Index (ISI)* [@bastien2001; @morin2011a] is a 7-item questionnaire to assess insomnia severity and its impact on the patient's life. Raters use a 5-point scale ranging from 0 (no problem) to 4 (very severe problem). We used the Brazilian-Portuguese version [@castro].

2. *The Hospital Anxiety and Depression Scale (HADS)* [@zigmond1983hospital] is a scale used to assess psychological distress in non-psychiatric patients. It is formed by a two-factor structure with seven items assessing Anxiety plus seven other items measuring Depression. A Brazilian-Portuguese version produced by @botega1995transtornos was used.

3. *Acceptance and Action Questionnaire-II (AAQ-II)* [@bond2011preliminary; @hayes2004measuring] is a measure of psychological flexibility composed by seven items rated in a scale from 1 (never true) to 7 (always true). it is scored by adding up scores for each question. Higher scoring indicate less flexibility. The Brazilian-portuguese version used in this study was produced by @barbosa2015propriedades.

## Analytical Plan

### Descriptive statistics

This phase comprise examination of response frequency and item statistics in order to assess item variation, distribution and data entry quality. Items with insufficient variation might be bad for differentiating respondents and may need to be excluded or merged into fewer categories [@dima2018]. We'll also diagnose inter-item correlations, scan for multivariate outliers (via Mahalanobis distance) to identify if there are any anomalous response patterns, and verify if the items follow a multivariate normal distribution.

### Non-parametric item response theory (NIRT)

Next, we examine item response patterns using Mokken Scaling Analysis (MSA), a non-parametric Item Response Theory (NIRT) technique. NIRT models provide a more flexible alternative to models from the parametric item response theory (PIRT) family, by employing less restrictive assumptions about the data. However pertaining both family of models, assumptions such as Local independence, Monotonicity and Unidimensionality can be weakened in such ways in NIRT, that an ordinal measurement is possible [@junker2001]. The MSA approach requires that the item response function (IRF) meet only ordering requirements, exempting the need to match a particular shape [@wind2017]. MSA allows ordering people on latent variable $\theta$ by their test scores; investigation of unidimensionality by identifying subscales and deviating items using the AISP algorithm; estimation of the item step response functions (ISRFs) and assessment of the non-monotonicities; and differential item functioning (i.e., if the same item have different response probabilities for people having the same $\theta$ level but who are members of two different groups) [@sijtsma2017]. To run the analyses we'll use the `mokken` package available in R [@ark2007; @ark2012]. explicar alguns termos

### Structural Validity Evidence

To assess the factorial structure of both measures, we will conduct Confirmatory Factor Analyses (CFAs) taking as a priori guess the aspects of the original models (i.e, the number of factors present in the data, which indicators are related to which factors, presence of higher-order or bi-factor structure etc.). Given that our data was collected using Likert scales (i.e., in ordered categories) and prior inspection of the data has shown skewed distribution for both DBAS-16 and SPAQ scores, we have opted to use the weighted least squares means and variance adjusted estimation (WLSMV) estimator based on the polychoric correlation matrix, since it does not rely on normal-theory [@brown2015]. To evaluate model fit we'll use the following fit statistics: chi-squared ($\chi^2$); Tucker-Lewis Index (TLI); Comparative Fit Index (CFI);  Relative Noncentrality Index (RNI); Root Mean Square Error of Approximation (RMSEA); and Standardized Root Mean Squared Residual (SRMR). 

Validation studies have traditionally relied on fixed cutoffs values such as SRMR $\le$ .08, RMSEA $\le$ .06, and CFI, TLI and RNI $\ge$ .96 to assess model misspecification [@hu1999]. Although used largely in CFA studies, the adoption of such criteria does not comes without its problems, as with much of other one-size-fits-all solutions used in applied psychological research [@mcneish2021]. Additionally, these cutoff values were established for continuous data analyzed using the normal-theory maximum likelihood (ML). Simulation studies have shown that for DWLS no universal cutoff values are appropriate; the number of categories and the threshold values might suggest different values for the indices [@xia2019]. Therefore, following @xia2019 recommendations, we'll interpret the aforementioned fit indices as diagnostic tools rather than a blind criteria for accepting or rejecting the hypothesized models.

We also plan to investigate measurement invariance across groups of good and bad sleepers (i.e., if psychometric properties of the scales are equivalent across groups). To attain this goal we'll use the multiple-group CFA (MGCFA) approach. The MGCFA approach tests invariance by constraining measurement properties (i.e., factor structure, factor loadings, intercepts, and residual variances) across groups in a series of increasingly restrictive models [@flake2021]. In each stage we test differences in fit between the restricted model and the less-restricted model looking at exact fit in terms of $\chi^2$ and degrees of freedom, CFI and RMSEA as well as the Akaike information criterion (AIC) and the Bayesian information criterion (BIC), where for those two lower values are an indication of better fit [@wicherts2010]. To conduct tests regarding factorial structure, the R package `lavaan` [@lavaan] will be used.

### Reliability Estimators

In Classical Test Theory (CTT) the reliability of a test is the ratio of true score variance to test score variance [@mcdonald1999]. The internal consistency of a scale is a test of reliability that measures the degree to which the set of items co-vary, relative to their sum score [@cronbach1951]. To indicate it we'll estimate Cronbach's alpha ($\alpha$), McDonald's omega total ($\omega_t$), and omega hierarchical ($\omega_h$). Although $\alpha$ is the most common measure of internal consistency reliability, $\omega_t$ (assumes an unidimensional scale) and $\omega_h$ (best for scales that may contain subfactors) are better alternatives because, contrary to $\alpha$, it does not assume tau equivalence (i.e., loadings are not assumeded to be equal) [@mcneish2018]. The internal consistency indices will be calculates using the R package `MBESS` [@MBESS]. Common guidelines suggests internal consistency indices $\ge$ .70 as an acceptable threshold for reliability [@kline1986].

In addition to internal consistency, we'll estimate the test–retest reliability to assess the consistency of test scores across time. This phase comprise a simple calculation of the Pearson product-moment correlation between baseline test scores and a second administration taken 14 days later. Higher correlation coefficients indicate higher test-retest reliability.

### Nomological net



### Calculation of global scores
