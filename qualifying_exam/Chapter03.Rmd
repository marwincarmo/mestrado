
# Method

## Participants and Study Design

```{r power-analysis}
ss_closefit_dbas <- semTools::findRMSEAsamplesize(rmsea0=.05, rmseaA=.059, df=98, power=.80)
ss_notclosefit_dbas <- semTools::findRMSEAsamplesize(rmsea0 = .08, rmseaA = .059, df=98, power=.80)

ss_closefit_spaq <- semTools::findRMSEAsamplesize(rmsea0=.05, rmseaA=.08, df=19, power=.80)
ss_notclosefit_spaq <- semTools::findRMSEAsamplesize(rmsea0 = .10, rmseaA = .08, df=19, power=.80)

```

To estimate an adequate sample size for the confirmatory factor analyses (CFAs), we used MacCallum et al.’s [-@maccallum1996] root-mean-square error of approximation (RMSEA) tests of close and not-close fit. All tests were conducted in R 4.1.3 [@R-base] using `semTools` version `r packageVersion("semTools")` [@semtools]. Morin [-@morin2007a] reports an RMSEA of 0.059 in a CFA for DBAS-16. Taking this value as a prior guess for the populational RMSEA value, we calculated the sample sizes required to reject the test for not-close fit of RMSEA > 0.08 and the test of close fit of RMSEA < 0.05 with a power of 0.80 and $\alpha$ of 0.05. Results show that `r ss_notclosefit_dbas` subjects are necessary to reject the test for not-close fit, and `r ss_closefit_dbas` participants are required to reject the test of close fit. Therefore, we aimed at a minimum sample size of `r ss_closefit_dbas` participants. SPAQ's fit index was not considered in this power analysis due to the large RMSEA (0.081) reported by the original publication [@bothelius2015].

The Ethics Committee of the General Hospital of the University of São Paulo, School of Medicine (HC-FMUSP), São Paulo, Brazil (CAAE: 46284821.1.0000.0068) approved this study. Inclusion criteria were age between 18 and 59 years and reported having no difficulties in reading or writing in Portuguese. 

Participants were recruited mainly from online advertisements, especially on HC-FMUSP's social media platforms (Instagram and Facebook). The data collection took place between May 2021 and July 2022, with brief breaks in between. Because the measures evaluated in this study refer to sleep difficulties, we sought to include participants with and without insomnia complaints. The first group was people registered for an experimental behavioral treatment for insomnia, which this study is a branch. To recruit participants without insomnia complaints, we asked for volunteer participation of people believing not to have sleeping problems.

Bad sleepers were classified according to the presence of insomnia complaints: (i) difficulty initiating and/or maintaining sleep, defined as a sleep onset latency and/or wake after sleep onset greater than or equal to 30 minutes, with a corresponding sleep time of less than or equal to six hours per night; (ii) presence of insomnia for more than three nights per week and more than three months; (iii) sleep disturbance (or associated daytime fatigue) causing significant distress or impairment in social, occupational, or other areas of functioning. This definition represents a combination of criteria from the American Academy of Sleep Medicine, the International Classification of Sleep Disorders, and the Diagnostic and Statistical Manual of Mental Disorders, along with quantitative cutoffs typically used in insomnia research [@icds2014; @americanpsychiatricassociation2013; @edinger2004]. In addition to these criteria, participants' total score on the Insomnia Severity Index should not exceed 7 points [@bastien2001].

Participants were informed about the main objective of the research and signed the informed consent. They were informed that their answers would be kept confidential and that all procedures guaranteeing the privacy of their results would be adopted. Then, they were requested to respond to an online survey using REDCap electronic data capture tools [@harris2009research; @harris2019redcap], including the Brazilian-Portuguese versions of DBAS-16 and SPAQ and other auxiliary instruments.

### Item translation

We mainly based our methods on Beaton's [-@beaton2000] recommendations with the addition of more up-to-date insights from @borsaAdaptacaoValidacaoInstrumentos2012. The following procedures were applied both to DBAS-16 as well as to SPAQ. Only the expert committee and the first translation team had a different configuration for each instrument.
Figure \@ref(fig:diag) summarizes the steps taken in the process.

In the first stage, the items of the original versions were translated from English (source language) to Portuguese (target language) by three independent translators. Two were familiar with the instrument constructs and the other, English teachers unaware of the instrument concepts and with no clinical or medical background. An expert committee of health professionals in insomnia synthesized the three versions. A form adapted from @koller2012 was given to each committee member to register the rationale for the decisions (see Appendix \ref{appendix:form}). Then, two independent translators, native speakers of the source language, back-translated the synthesized version to English. We reconciled the back translations into a single version and submitted it to appreciation by both first authors of the original questionnaires. Together with the expert committee, we debated over suggestions raised by the original authors and made changes accordingly to the translated version. 

In the final step, we conducted a pilot study with 15 participants from the target population to probe the pre-final version. There were 12 female participants, and the overall mean age was 43 years (range: 19--57 years). To obtain feedback from diverse regional contexts [@borsaAdaptacaoValidacaoInstrumentos2012], we recruited participants from the five Brazilian regions and with varying educational levels. We could interview nine participants from the Southwest region, three from the South, two from the Northeast, and one from Middle-west. We conducted individual cognitive interviews with each participant.

```{r diag, fig.cap='Stages of cross-cultural adaptation. Adapted from "Cross-Cultural Adaptation and Validation of Psychological Instruments: Some Considerations", by J. C. Borsa, B. F. Damásio and D. R. Bandeira, 2012, Paidéia, 22(53), 423-432; "Guidelines for the Process of Cross-Cultural Adaptation of Self-Report Measures", by D. E. Beaton, C. Bombardier, F. Guillemin, and M. B. Ferraz, 2000, SPINE, 25(24), 3186-3191.', fig.height=14, dpi=300, fig.align='center'}

webshot::webshot(url = "diag.html",
                 file= "diag.png")
```

```{r flowcca2, eval=FALSE, fig.align='center', fig.cap='Stages of cross-cultural adaptation. Adapted from “Cross-Cultural Adaptation and Validation of Psychological Instruments: Some Considerations”', fig.height=8, fig.width=10, message=FALSE, include=FALSE, out.width="100%"}
DiagrammeR::grViz("digraph {
  graph [layout = dot, rankdir = TB]
  
  node [shape = rectangle] 
  init [label = 'Original instrument']
  ft1 [label = 'Translator 1 \n(informed translator)']
  ft2 [label = 'Translator 2 \n(informed translator)']
  ft3 [label = 'Translator 3 \n(uninformed translator)']
  synth [label = 'Synthesyzed \ntranslation']
  expcom [label = 'Expert committee \nreview']
  bkt1 [label = 'Backtranslator 1 \n(uninformed translator)']
  bkt2 [label = 'Backtranslator 2 \n(uninformed translator)']
  synthb [label = 'Synthesyzed \nbacktranslation']
  pog [label = 'Presentation of the new version \nto the original authors']
  adq [label = 'Adequate?']
  disc [label = 'Discussion with the \noriginal authors']
  
  
  # edge definitions with the node IDs
  init -> {ft1 ft2 ft3}
  {ft1 ft2 ft3} -> synth
  synth -> expcom -> {bkt1 bkt2}
  {bkt1 bkt2} -> synthb
  synthb -> pog
  {rank=same; pog; adq}
  pog -> adq 
  adq -> {Yes No}
  {rank=same; No; disc}
  No -> disc -> adq
  Yes -> 'Pilot Study' -> 'Expert committee \nreview' -> 'Final translated \n version'
  }",
  height = 1680, 
  width = 800
  )
```


## Additional measures

1. *Insomnia Severity Index (ISI)* [@bastien2001; @morin2011a] is a 7-item questionnaire to assess insomnia severity and its impact on the patient's life. Raters use a 5-point scale ranging from 0 (no problem) to 4 (very severe problem). We used the Brazilian-Portuguese version [@castro].

2. *The Hospital Anxiety and Depression Scale (HADS)* [@zigmond1983hospital] assesses psychological distress in non-psychiatric patients. It contains a two-factor structure with seven items assessing Anxiety plus seven other items measuring Depression. A Brazilian-Portuguese version produced by @botega1995transtornos was used.

3. *Acceptance and Action Questionnaire-II (AAQ-II)* [@bond2011preliminary; @hayes2004measuring] is a measure of psychological flexibility composed of seven items rated on a scale from 1 (never true) to 7 (always true). It is scored by adding up scores for each question. Higher scoring indicate low psychological flexibility. The Brazilian-Portuguese version used in this study was produced by @barbosa2015propriedades.

## Analytical Plan

### Descriptive statistics

This phase examines of response frequency and item statistics to assess item variation, distribution, and data entry. Items with insufficient variation might be inadequate for differentiating respondents and may need to be excluded or merged into fewer categories [@dima2018]. We will also diagnose inter-item correlations, scan for multivariate outliers (via Mahalanobis distance) to identify any anomalous response patterns and verify if the items follow a multivariate normal distribution.

### Non-parametric item response theory (NIRT)

Next, we examine item response patterns using Mokken Scaling Analysis (MSA), a non-parametric Item Response Theory (NIRT) technique. NIRT models provide a more flexible alternative to models from the parametric item response theory (PIRT) family by employing less restrictive assumptions about the data. However pertaining to both models, assumptions such as Local independence, Monotonicity, and Unidimensionality can be weakened in such ways in NIRT that an ordinal measurement is possible [@junker2001]. The MSA approach requires that the item response function (IRF) meet only ordering requirements, exempting the need to match a particular shape [@wind2017]. MSA allows ordering people on latent variable $\theta$ by their test scores; investigating unidimensionality by identifying subscales and deviating items using the AISP algorithm; estimation of the item step response functions (ISRFs; i.e., the probability of obtaining at least score $x_j$ given a latent variable $\theta$, $P(X_j \ge x_j|\theta)$) and assessment of the non-monotonicities (i.e., probability of endorsing a 'correct' response option not increasing with increasing levels of the latent dimension); and differential item functioning (i.e., if the same item have different response probabilities for people having the same $\theta$ level but who are members of two different groups) [@sijtsma2017]. To run the analyses, we will use the `mokken` package available in R [@ark2007; @ark2012].

### Structural Validity Evidence

To assess the factorial structure of both measures, we will conduct Confirmatory Factor Analyses (CFAs) taking as a priori guess the aspects of the original models (i.e, the number of factors present in the data, which indicators are related to which factors, presence of higher-order or bi-factor structure etc.). Given that our data was collected using Likert scales with more than six ordered categories, we have opted to follow recent suggestions to use the Maximum Likelihood with Robust standard errors (MLR) estimator [@rhemtulla2012]. To evaluate model fit, we will use the following fit statistics: chi-squared ($\chi^2$); Tucker-Lewis Index (TLI); Comparative Fit Index (CFI);  Relative Noncentrality Index (RNI); Root Mean Square Error of Approximation (RMSEA); and Standardized Root Mean Squared Residual (SRMR). 

CFA studies have traditionally relied on fixed cutoff values such as SRMR $\le$ .08, RMSEA $\le$ .06, and CFI, TLI, and RNI $\ge$ .96 to assess model misspecification [@hu1999]. Although used broadly, adopting such criteria does not come without its problems, as with many other one-size-fits-all solutions used in applied psychological research [@mcneish2021]. Moreover, these cutoff values were established for continuous data analyzed using the normal-theory maximum likelihood (ML). Therefore, following Xia and Yang's [-@xia2019] recommendations, we will interpret these fit indices as diagnostic tools rather than a blind criterion for accepting or rejecting the hypothesized models.

We also plan to investigate measurement invariance across groups with and without insomnia symptoms (i.e., if the psychometric properties of the scales are equivalent across groups). To attain this goal we'll use the multiple-group CFA (MGCFA) approach. The MGCFA approach tests invariance by constraining measurement properties (i.e., factor structure, factor loadings, intercepts, and residual variances) across groups in a series of increasingly restrictive models [@flake2021]. In each stage we test differences in fit between the restricted model and the less-restricted model looking at exact fit in terms of $\chi^2$ and degrees of freedom, CFI and, RMSEA [@wicherts2010]. We also look at the Akaike information criterion (AIC) and the Bayesian information criterion (BIC), where, for those two, lower values are an indication of better fit. We will use the R package `lavaan` [@lavaan] to conduct these tests.

### Reliability Estimators

In Classical Test Theory (CTT), the reliability of a test is the ratio of true score variance to test score variance [@mcdonald1999]. The internal consistency of a scale is a test of reliability that measures the degree to which the set of items co-vary, relative to their sum score [@cronbach1951]. To indicate it we'll estimate Cronbach's alpha ($\alpha$), McDonald's omega total ($\omega_t$), and omega hierarchical ($\omega_h$). Although $\alpha$ is the most common measure of internal consistency reliability, $\omega_t$ (assumes an unidimensional scale) and $\omega_h$ (best for scales that may contain subfactors) are better alternatives because, contrary to $\alpha$, it does not assume tau equivalence (i.e., loadings are not assumeded to be equal) [@mcneish2018]. The internal consistency indices will be calculated using the R package `MBESS` [@MBESS]. Common guidelines suggests internal consistency indices $\ge$ .70 as an acceptable threshold for reliability [@kline1986].

In addition to internal consistency, we will estimate the test-retest reliability to assess the consistency of test scores across time. This phase comprises a simple calculation of the Pearson product-moment correlation between baseline test scores and a second administration taken 14 days later. Higher correlation coefficients indicate high test-retest reliability.

### Convergent validity evidence

Convergent validity refers to the expected relationship between test scores and other measures of the same or similar constructs [@standards2014]. Regarding the DBAS-16, we expect a positive correlation between its scores and the HADS subscales of depression and anxiety and with IGI scores. For SPAQ, the same relationships are expected with the addition of also showing a negative correlation with AAQ-II scores.

### Network psychometrics

Psychiatry and psychology have recently begun to shift from viewing psychopathology as originating from a root cause (e.g., latent variable models) to an approach that models it as a network of causal interactions among symptoms [@borsboom2008; @borsboom2013; @bringmann2022]. In line with these recent formulations, we will conduct exploratory analyses of the scales subject of this study following a psychometric network perspective. We will conduct this phase following the steps outlined by @christensen2020b to test validity from the network perspective: a) Redundancy Analysis, b) Dimension Analysis, and c) Internal Structure Analysis. All these analyses will be performed using the `EGAnet` [@EGAnet] package for R.

Redundant items can cause unintended effects when estimating dimensionality in psychometric modeling. The shared substantive cause may obscure the interpretation of centrality measures in network models. For latent variable models, it has the potential to cause a violation of the principle of local independence, resulting in a poor fit [@christensen2020a]. To detect redundant items, we can use the *Unique Variable Analysis* (UVA) [@christensen2020a]. This algorithm first computes the association structure of the observed data, then uses a threshold or significance test to determine redundancy between pairs of variables. The redundant variables can be removed, leaving only one non-redundant indicator, or aggregated as latent variables.

To estimate dimensionality, we take advantage of a popular technique in the psychometric network literature called *Exploratory Graph Analysis* (EGA) [@golino2017]. This method estimates the number of dimensions in multivariate data using undirected network models. The EGA algorithm first estimates a Gaussian Graphical Model, using the graphical least absolute shrinkage and selection operator (GLASSO), then applies the Walktrap community detection algorithm to determine the number and content of communities in the network. These communities in network models are statistically equivalent to factors of latent variable models [@golino2017].

A network equivalent of factor loadings is a measure called *network loading* [@christensen2021]. These loadings represent the unique contribution of each node to the emergence of a dimension in a network. The authors of this method also argue that network and factor loadings are comparable when the data-generating model is a factor model.

Computing internal consistency measures from a network perspective is not possible because the necessary common covariance between items is removed in network models [@christensen2020b]. To overcome this limitation, @christensen2020b suggests examining "the extent to which items in a dimension are homogeneous and interrelated given the multidimensional structure of the questionnaire" (p. 8), which they refer to as *structural consistency*. @christensen2021a developed the *Bootstrap Exploratory Graph Analysis* (bootEGA) to estimate this measure. In rough terms, bootEGA generates a sampling distribution of EGA results using simulated data to inform how often dimensions are replicated across the bootstrap replicates (structural consistency) and how often each item is allocated in its respective empirical dimension across replications (item stability).